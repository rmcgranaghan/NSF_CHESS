{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As part of the CHESS workshop, we will be simulating a space weather event. We model this event on the 2003 Halloween storm, and first obtain the relevant \"ground truth\" data for that time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with hourly-averaged ASCII data obtained from the [NASA OMNI database](https://omniweb.gsfc.nasa.gov/form/dx1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"../raw_data\")\n",
    "header_file = data_folder / \"omni_2003_header.txt\"\n",
    "data_file = data_folder / \"omni_2003.asc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../raw_data/omni2_2003_header.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58187/2400259324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reader header info into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mheader_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../raw_data/omni2_2003_header.txt'"
     ]
    }
   ],
   "source": [
    "# Reader header info into a list\n",
    "header_info = []\n",
    "with open(header_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "header_info = [x.strip().split()[1] for x in lines[4:]]\n",
    "print(header_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into a dataframe and provide header as column names\n",
    "# Infer column structure by using all of the lines in the file\n",
    "df = pd.read_fwf(data_file, names=header_info, infer_nrows = 8760, index_col=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce a datetime colum, and remove the year, DOY and hour columns\n",
    "df[\"datetime\"] = pd.to_datetime(df['YEAR'] * 100000 + 100*df['DOY']+df['Hour'], format='%Y%j%H')\n",
    "new_index = ['datetime'] + df.keys().tolist()[3:-1]\n",
    "df = df[new_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we're interested in the 2003 Halloween storm period as our baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(year=2003, month=10, day=25)\n",
    "end_date = datetime.datetime(year=2003, month=11, day=8)\n",
    "\n",
    "halloween_storm = df[df['datetime'].between(start_date, end_date, inclusive=True)]\n",
    "halloween_storm.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halloween_storm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halloween_storm_plus = halloween_storm.copy()\n",
    "halloween_storm_minus = halloween_storm.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate two additional dataframes, one enhanced and one diminished version of the storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical quantities we scale by a factor of 5\n",
    "scaling_factor = 5\n",
    "for key in ['BX', 'BY', 'BZ', 'SW_Plasma_Temperature', 'SW_Proton_Density']:\n",
    "        halloween_storm_plus[key] = scaling_factor*halloween_storm_plus[key] - np.nanmean(scaling_factor*halloween_storm_plus[key])\n",
    "\n",
    "halloween_storm_plus['Scalar_B'] = np.sqrt(halloween_storm_plus['BX']**2 + halloween_storm_plus['BY']**2 + halloween_storm_plus['BZ']**2)        \n",
    "\n",
    "# These are indices, and do not the same ways as physical quantities.\n",
    "# We arbitrarily scale them by 1.25\n",
    "scaling_factor = 1.25\n",
    "for key in ['Kp', 'ap_index', 'f10.7_index', 'AE-index']:\n",
    "    halloween_storm_plus[key] = scaling_factor*halloween_storm_plus[key]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical quantities we scale by a factor of 5\n",
    "scaling_factor = .5\n",
    "for key in ['BX', 'BY', 'BZ', 'SW_Plasma_Temperature', 'SW_Proton_Density']:\n",
    "        halloween_storm_minus[key] = scaling_factor*halloween_storm_minus[key] - np.nanmean(scaling_factor*halloween_storm_minus[key])\n",
    "\n",
    "halloween_storm_minus['Scalar_B'] = np.sqrt(halloween_storm_minus['BX']**2 + halloween_storm_minus['BY']**2 + halloween_storm_minus['BZ']**2)        \n",
    "\n",
    "# These are indices, and do not the same ways as physical quantities.\n",
    "# We arbitrarily scale them by 1.25\n",
    "scaling_factor = .75\n",
    "for key in ['Kp', 'ap_index', 'f10.7_index', 'AE-index']:\n",
    "    halloween_storm_minus[key] = scaling_factor*halloween_storm_minus[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(halloween_storm['datetime'], halloween_storm['BZ'],label = 'Orig')\n",
    "plt.plot(halloween_storm_plus['datetime'],halloween_storm_plus['BZ'], label = 'Enhanced')\n",
    "plt.plot(halloween_storm_minus['datetime'], halloween_storm_minus['BZ'], label = 'Diminished')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Bz component of solar wind for Halloween storm\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create netCDF files so that these datasets can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = xarray.Dataset.from_dataframe(halloween_storm)\n",
    "xr['BX'].attrs={'units':'nT'}\n",
    "xr['BY'].attrs={'units':'nT'}\n",
    "xr['BZ'].attrs={'units':'nT'}\n",
    "xr['Scalar_B'].attrs={'units':'nT'}\n",
    "xr['SW_Plasma_Temperature'].attrs={'units':'K'}\n",
    "xr['SW_Proton_Density'].attrs={'units':'cm^-3'}\n",
    "xr['ap_index'].attrs={'units':'nT'}\n",
    "xr['AE-index'].attrs={'units':'nT'}\n",
    "\n",
    "\n",
    "xr.to_netcdf(data_folder / 'halloween_storm.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = xarray.Dataset.from_dataframe(halloween_storm_plus)\n",
    "xr['BX'].attrs={'units':'nT'}\n",
    "xr['BY'].attrs={'units':'nT'}\n",
    "xr['BZ'].attrs={'units':'nT'}\n",
    "xr['Scalar_B'].attrs={'units':'nT'}\n",
    "xr['SW_Plasma_Temperature'].attrs={'units':'K'}\n",
    "xr['SW_Proton_Density'].attrs={'units':'cm^-3'}\n",
    "xr['ap_index'].attrs={'units':'nT'}\n",
    "xr['AE-index'].attrs={'units':'nT'}\n",
    "\n",
    "\n",
    "xr.to_netcdf(data_folder / 'halloween_storm_plus.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = xarray.Dataset.from_dataframe(halloween_storm_minus)\n",
    "xr['BX'].attrs={'units':'nT'}\n",
    "xr['BY'].attrs={'units':'nT'}\n",
    "xr['BZ'].attrs={'units':'nT'}\n",
    "xr['Scalar_B'].attrs={'units':'nT'}\n",
    "xr['SW_Plasma_Temperature'].attrs={'units':'K'}\n",
    "xr['SW_Proton_Density'].attrs={'units':'cm^-3'}\n",
    "xr['ap_index'].attrs={'units':'nT'}\n",
    "xr['AE-index'].attrs={'units':'nT'}\n",
    "\n",
    "\n",
    "xr.to_netcdf(data_folder / 'halloween_storm_minus.nc')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11cf427ed60813550d04719e2cd25fddd09bfe6efc07d905d21ecaf5bacfb0c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('chess': conda)",
   "language": "python",
   "name": "python3101jvsc74a57bd011cf427ed60813550d04719e2cd25fddd09bfe6efc07d905d21ecaf5bacfb0c9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
